{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Working Example: CE+NTL Loss Function\n",
    "\n",
    "This notebook demonstrates a minimal working example for training a decoder only Hugging Face language model using the NTL+CE loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "from ntl.tokenizer.abstract_tokenizer import NumberEncodingTokenizer\n",
    "from ntl.data.data import load_txt_dataset\n",
    "from ntl.loss_functions.base_number_token_loss import CEWithNTL\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up custom collator for decoder only models\n",
    "\n",
    "\n",
    "class LlamaQACollator(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, tokenizer: NumberEncodingTokenizer):\n",
    "        super().__init__(tokenizer, mlm=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, Union[str, List[int]]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Merge questions and answers into single sequences (needed for decoder only)\n",
    "        merged_sequences = [f\"{ex['question']}{ex['answer']}\" for ex in examples]\n",
    "\n",
    "        # Tokenize merged sequences\n",
    "        batch = self.tokenizer(\n",
    "            merged_sequences, padding=True, truncation=True, return_tensors=\"pt\", return_attention_mask=True\n",
    "        )\n",
    "\n",
    "        # Masking questions to create labels\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "        for i, ex in enumerate(examples):\n",
    "            # Find question length in tokens\n",
    "            question_tokens = self.tokenizer(\n",
    "                ex[\"question\"],\n",
    "                return_tensors=\"pt\",\n",
    "                add_special_tokens=False,\n",
    "            )\n",
    "            question_length = question_tokens[\"input_ids\"].size(1)\n",
    "\n",
    "            # Mask question tokens and preserve answer tokens\n",
    "            labels[i, :question_length] = -100\n",
    "\n",
    "        # Mask padding tokens\n",
    "        labels[labels == self.pad_token_id] = -100\n",
    "\n",
    "        return {\"input_ids\": batch[\"input_ids\"], \"attention_mask\": batch[\"attention_mask\"], \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model and tokenizer\n",
    "\n",
    "model_name = \"tinyllama/tinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset and collator\n",
    "\n",
    "dataset = load_txt_dataset(\"data/mathematics_dataset-v1.0/arithmetic_val.txt\")\n",
    "data_collator = LlamaQACollator(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the trainer with the integrated NTL+CE loss function\n",
    "\n",
    "ce_with_ntl = CEWithNTL(tokenizer=tokenizer, ntl_weight=0.3)\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Get model outputs\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # Compute loss\n",
    "        loss = ce_with_ntl(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 2 steps\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama2-finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    max_steps=2,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    use_cpu=True,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
