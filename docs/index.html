<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Meta -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Regress, Don’t Guess – Number Token Loss</title>

    <!-- Fonts & CSS -->
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@300;400;600&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css">
    <link rel="stylesheet" href="assets/css/styles.css">

    <!-- Icons & Favicon -->
    <link rel="icon" href="assets/favicon.png" type="image/png">

    <!-- Social/SEO -->
    <meta property="og:site_name" content="Number Token Loss">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Regress, Don’t Guess – Number Token Loss">
    <meta property="og:description" content="A regression-like loss on number tokens for language models.">
    <meta property="og:url" content="https://tum-ai.github.io/number-token-loss">
    <meta property="og:image" content="assets/thumbnail.png">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Regress, Don’t Guess – Number Token Loss">
    <meta name="twitter:description" content="A regression-like loss on number tokens for language models.">
    <meta name="twitter:image" content="assets/thumbnail.png">

    <!-- Optional analytics -->
    <!--
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXX"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date()); gtag('config', 'G-XXXXXXX');
    </script>
    -->
    <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

    <!-- ▸▸▸ Header ─────────────────────────────────────────────────────── -->
    <header class="text-center py-4 bg-light">
        <div class="container">
            <h1 class="display-4 font-weight-bold">Regress, Don’t Guess<br><small class="h2">A Regression-like Loss on
                    <span class="text-primary">N</span>umber <span class="text-primary">T</span>okens for <span
                        class="text-primary">L</span>anguage Models</small></h1>
            <h3 class="mt-3">
                <img src="assets/ICML-logo.svg" alt="ICML logo"
                    style="height: 2.5em; vertical-align: middle; margin-right: 0.3em;">
                <span style="font-size: 1.5em;">2025</span>
            </h3>
            <!-- Author grid -->
            <div class="row justify-content-center mt-4">
                <!-- Repeat “col-md-2” blocks as needed; five per row works nicely -->
                <div class="col-10 col-sm-3 col-md-2">
                    <h6><a href="https://scholar.google.com/citations?user=JPs9ufIAAAAJ&hl=en&oi=ao">Jonas Zausinger</a>
                    </h6>
                    <p class="small">TUM.ai / TUM</p>
                </div>

                <div class="col-10 col-sm-3 col-md-2">
                    <h6><a href="https://scholar.google.com/citations?user=CIjU6UgAAAAJ&hl=en&oi=ao">Lars Pennig</a>
                    </h6>
                    <p class="small">TUM.ai / TUM / Helmholtz / MCML</p>
                </div>

                <div class="col-10 col-sm-3 col-md-2">
                    <h6>Anamarija Kozina</h6>
                    <p class="small">TUM.ai / TUM</p>
                </div>

                <div class="col-10 col-sm-3 col-md-2">
                    <h6>Sean Sdahl</h6>
                    <p class="small">TUM.ai / TUM</p>
                </div>

                <div class="col-10 col-sm-3 col-md-2">
                    <h6>Julian Sikora</h6>
                    <p class="small">TUM.ai / TUM</p>
                </div>



                <div class="col-10 col-sm-3 col-md-2">
                    <h6><a href="https://scholar.google.com/citations?user=Flm5HnwAAAAJ&hl=en&oi=ao">Adrian
                            Dendorfer</a></h6>
                    <p class="small">TUM.ai / TUM</p>
                </div>
                <div class="col-10 col-sm-3 col-md-2">
                    <h6>Timofey Kuznetsov</h6>
                    <p class="small">TUM.ai / TUM</p>
                </div>
                <div class="col-10 col-sm-3 col-md-2">
                    <h6><a href="https://scholar.google.com/citations?user=UldC_xsAAAAJ&hl=en&oi=ao">Mohamad Hagog</a>
                    </h6>
                    <p class="small">TUM.ai / LMU</p>
                </div>
                <div class="col-10 col-sm-3 col-md-2">
                    <h6><a href="https://scholar.google.com/citations?user=qC1JKzoAAAAJ&hl=en&oi=ao">Nina Wiedemann</a>
                    </h6>
                    <p class="small">ETH Zurich</p>
                </div>
                <div class="col-10 col-sm-3 col-md-2">
                    <h6>Kacper Chlodny</h6>
                    <p class="small">TUM.ai / TUM</p>
                </div>
                <div class="col-10 col-sm-3 col-md-2">
                    <h6>Vincent Limbach</h6>
                    <p class="small">TUM.ai / TUM</p>
                </div>
                <div class="col-10 col-sm-3 col-md-2">
                    <h6>Anna Ketteler</h6>
                    <p class="small">TUM.ai / TUM</p>
                </div>
                <div class="col-10 col-sm-3 col-md-2">
                    <h6><a href="https://scholar.google.com/citations?hl=en&user=qbX8sKwAAAAJ">Thorben Prein</a></h6>
                    <p class="small">TUM.ai / TUM</p>
                </div>
                <div class="col-10 col-sm-3 col-md-2">
                    <h6><a href="https://scholar.google.com/citations?hl=en&user=u5iBnt0AAAAJ">Vishwa Mohan Singh</a>
                    </h6>
                    <p class="small">TUM.ai / LMU</p>
                </div>
                <div class="col-10 col-sm-3 col-md-2">
                    <h6><a href="https://scholar.google.com/citations?hl=en&user=60LxTqgAAAAJ">Michael Danziger</a></h6>
                    <p class="small">IBM Research Europe</p>
                </div>
                <div class="col-10 col-sm-3 col-md-2">
                    <h6><a href="https://scholar.google.com/citations?hl=en&user=FHL-zfsAAAAJ">Jannis Born</a></h6>
                    <p class="small">IBM Research Europe</p>
                </div>

                <!-- …continue for all co-authors, or cut after ~10 and add “et al.” -->
            </div>

            <!-- Action buttons -->
            <p class="lead mt-3">
                <a href="https://ibm.biz/ntl-paper" class="btn btn-outline-dark mx-1" style="font-size: 1.1em;">
                    <img src="assets/ICML-head.svg" alt="ICML logo"
                        style="height: 2em; vertical-align: middle; margin-right: 0.3em;">
                    Paper
                </a>
                <a href="https://ibm.biz/ntl-pypi-repo" class="btn btn-outline-dark mx-1" style="font-size: 1.1em;">
                    <img src="assets/pypi.png" alt="PyPI logo"
                        style="height: 2em; vertical-align: middle; margin-right: 0.3em;">
                    <i class="fab fa-github"
                        style="font-size: 2em; vertical-align: middle; margin-right: 0.3em;"></i>PyPI Code</a>
                <a href="https://ibm.biz/ntl-code" class="btn btn-outline-dark mx-1" style="font-size: 1.1em;"><i
                        class="fab fa-github" style="font-size: 2em; vertical-align: middle; margin-right: 0.3em;"></i>
                    Dev. Code</a>
                <a href="https://ibm.biz/ntl-5min-yt" class="btn btn-outline-dark mx-1" style="font-size: 1.1em;">
                    <i class="fab fa-youtube"
                        style="font-size: 2em; vertical-align: middle; margin-right: 0.3em; color:#FF0000;"></i>
                    ICML Talk
                </a>
                <a href="https://ibm.biz/ntl-demo" class="btn btn-outline-dark mx-1" style="font-size: 1.1em;">
                    <img src="assets/streamlit.png" alt="Streamlit logo"
                        style="height: 2em; vertical-align: middle; margin-right: 0.3em;">Demo</a>
            </p>
        </div>
    </header>

    <section class="py-2" id="figure">
        <div class="container">
            <h5 class="text-center">The <span class="text-primary">N</span>umber <span class="text-primary">T</span>oken
                <span class="text-primary">L</span>oss (<span class="text-primary">NTL</span>) augments cross-entropy to
                improve Language Models in numerical tasks.
            </h5>
        </div>
        <figure class="text-center my-2">
            <img src="assets/ntl_v5.svg" alt="Number Token Loss schematic" class="img-fluid mx-auto d-block"
                style="width: 100%; max-width: 400px; height: auto;">
            <figcaption class="mt-1 small text-muted" style="font-size: 1.05em;">
                Number Token Loss (NTL) is the Wasserstein-1 distance between the predicted and real distribution over
                the number tokens.
            </figcaption>
        </figure>
    </section>
    <!-- ▸▸▸ Abstract ────────────────────────────────────────────────────── -->
    <section class="py-2 bg-light" id="abstract">
        <div class="container">
            <h2>Abstract</h2>
            <p class="text-justify">
                While language models have exceptional capabilities at text generation, they lack a natural inductive
                bias
                for emitting numbers and thus struggle in tasks involving quantitative reasoning, especially arithmetic.
                One fundamental limitation is the nature of the Cross Entropy loss, which assumes a nominal scale and
                thus
                cannot convey proximity between generated number tokens.
                In response, we here present a regression-like loss that operates purely on token level.
                Our proposed <span class="text-primary">N</span>umber <span class="text-primary">T</span>oken <span
                    class="text-primary">L</span>oss (NTL) comes in two flavors and minimizes either the
                <em>L<sub>p</sub></em> norm or the Wasserstein distance between the
                <em>numerical values</em> of the real and predicted number tokens.
                NTL can easily be added to any language model and extend the Cross Entropy objective during training
                without runtime overhead.
                We evaluate the proposed scheme on various mathematical datasets and find that it consistently improves
                performance in math-related tasks.
                In a direct comparison on a regression task, we find that NTL can match the performance of a regression
                head, despite operating on token level.
                Finally, we scale NTL up to 3B parameter models and observe improved performance, demonstrating its
                potential for seamless integration into LLMs.
                We hope that this work can inspire LLM developers to improve their pretraining objectives.
            </p>
        </div>
    </section>

    <!-- ▸▸▸ Method / Figure 1 ------------------------------------------- -->
    <section class="py-3">
        <div class="container">
            <h2>Why do we need the Number Token Loss (NTL)?</h2>
            <p class="text-justify">
                Cross Entropy is <strong>nominal-scale</strong> and thus assigns equal loss to all incorrect
                predictions. This makes sense for normal tokens but not for number tokens:
            </p>

            <div class="text-center my-3">
                <strong>
                    With a ground truth token <code>4</code>, predicting <code>3</code> or <code>9</code> should not
                    give equal loss 🤔😱<br>NTL fixes this! 🚀💪
                </strong>
            </div>

            <p class="text-justify">
                For all number tokens, NTL increases with distance from ground truth just like a regression loss.
                But it doesnt need an extra head, it allows computing a regression-like loss directly on a token head.
                We propose two schemes:
                <br>
                <strong>NTL-WAS</strong> – Wasserstein-1 distance between predicted and one-hot number distributions
                (see plot above).<br>
                <strong>NTL-MSE</strong> – Dot-product expectation of numeric value with squared error (most intuitive
                but has some undesired local minima)
            </p>
            <figure class="text-center my-1">
                <img src="assets/loss_comparison_v4.svg" alt="Number Token Loss VS. Cross Entropy" class="img-fluid"
                    style="max-width:500px">
                <!-- <figcaption class="mt-2 small text-muted">Comparison of NTL to CE.</figcaption> -->
            </figure>
        </div>
    </section>
    <!-- ▸▸▸ Results summary (optional quick bullets) -------------------- -->
    <section class="py-3 bg-light">
        <div class="container">
            <h2>Key Contributions & Results</h2>
            <ul>
                <li><strong>Model-agnostic:</strong> NTL is just a loss &rightarrow; applicable to any LM (e.g.,
                    Transformer, Mamba) in any architecture (encoder-decoder, decoder-only).</li>
                <li><strong>Plug-and-play:</strong> NTL requires only a mapping from tokens to numeric values and works
                    with digit-level and multi-digit tokenizations.</li>
                <li><strong>No computational overhead:</strong> NTL adds only ~1% compute time to <emph>loss calculation
                    </emph> which is negligible over a full training step.</li>
                <li><strong>Consistently improves performance:</strong> NTL outperforms plain cross entropy across
                    multiple architectures and math benchmarks.</li>
                <li><strong>Performs true regression:</strong> On regression tasks a LM head with NTL matches a
                    dedicated regression head.</li>
                <li><strong>Does not harm text tasks:</strong> On pure text tasks, NTL has zero effect, thus it behaves
                    like good old cross entropy.</li>
                <li><strong>Scales to large models:</strong> Even <a
                        href="https://huggingface.co/ibm-granite/granite-3.2-2b-instruct">Granite 3.2 2B</a> and <a
                        href="https://huggingface.co/google-t5/t5-3b">T5-3B</a> benefit heavily from NTL on math tasks
                    like GSM8K.</li>
            </ul>
        </div>
    </section>

    <!--
<section class="py-5">
    <div class="container">
        <h2>NTL at a Glance</h2>
        <p>
            The <strong>Number Token Loss (NTL)</strong> augments the cross-entropy loss to take the <em>numeric distance</em> between predicted and target number tokens into account.
            It is applied only to number tokens and is added to the usual token-level loss:
        </p>
        <p class="text-center mt-4 mb-4">
            \( \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{CE}} + \lambda \cdot \mathcal{L}_{\text{NTL}} \)
        </p>

        <p>We propose two variants of NTL:</p>

        <h5 class="mt-4"><strong>1. NTL-MSE:</strong> (Expectation-based squared error)</h5>
        <p>
            Interprets the number token distribution \( p \in \mathbb{R}^{|V_{\text{num}}|} \) as a soft prediction over values \( v \in \mathbb{R}^{|V_{\text{num}}|} \), and minimizes the squared error to the ground truth number \( y \in \mathbb{R} \):
        </p>
        <p class="text-center mt-3 mb-4">
            \( \mathcal{L}_{\text{NTL-MSE}} = \left( \sum_{i=1}^{|V_{\text{num}}|} p_i \cdot v_i - y \right)^2 \)
        </p>

        <h5 class="mt-4"><strong>2. NTL-WAS:</strong> (Wasserstein-1 distance)</h5>
        <p>
            Treats the target number token as a one-hot distribution and computes the discrete Wasserstein-1 (Earth Mover’s) distance between it and the predicted token distribution:
        </p>
        <p class="text-center mt-3 mb-4">
            \( \mathcal{L}_{\text{NTL-WAS}} = \sum_{i=1}^{|V_{\text{num}}|} \left| \text{CDF}_p(i) - \text{CDF}_q(i) \right| \)
        </p>
        <p class="text-muted small text-center">(where \( \text{CDF}_p \) and \( \text{CDF}_q \) are the cumulative distributions of the predicted and target distributions)</p>

        <p class="mt-4">
            Both losses can be implemented with negligible overhead and improve numeric reasoning in LMs. For full details, see Section 3 of the paper.
        </p>
    </div>
</section>
-->

    <!-- ▸▸▸ Placeholder for extra figures or benchmarks ----------------- -->
    <!--
<section class="py-5">
  <div class="container">
    <h2>Additional Figures</h2>
    <div class="row">
      <div class="col-md-6 text-center mb-4">
        <img src="assets/runtime_plot.svg" class="img-fluid" alt="Runtime">
        <p class="small mt-2">Figure 2: Marginal compute overhead of NTL.</p>
      </div>
      <div class="col-md-6 text-center mb-4">
        <img src="assets/maths_results.svg" class="img-fluid" alt="Math accuracy">
        <p class="small mt-2">Figure 3: Accuracy gains on maths tasks.</p>
      </div>
    </div>
  </div>
</section>
-->

    <section class="py-2 bg-light" id="pypi">
        <div class="container">
            <h2>Usage</h2>
            <p>
                NTL is available via the lightweight <code>ntloss</code> package from
                <a href="https://ibm.biz/ntl-pypi-repo">PyPI</a>.
            </p>
            <p>
                <code>ntloss</code> is in <strong>alpha</strong> stage and feedback &amp; PRs are welcome!
            </p>
        </div>
    </section>


    <!-- ▸▸▸ Citation ----------------------------------------------------- -->

    <section class="py-2 bg-light" id="citation">
        <div class="container">
            <h2>Citation</h2>
            <pre class="bg-white p-3 rounded">
@inproceedings{zausinger2025regress,
  title   = {Regress, Don't Guess – A Regression-like Loss on Number Tokens for Language Models},
  author  = {Jonas Zausinger and Lars Pennig and Anamarija Kozina and Sean Sdahl
             and Julian Sikora and Adrian Dendorfer and Timofey Kuznetsov
             and Mohamad Hagog and Nina Wiedemann and Kacper Chlodny
             and Vincent Limbach and Anna Ketteler and Thorben Prein
             and Vishwa Mohan Singh and Michael Danziger and Jannis Born},
  booktitle = {Proc. of the 42nd International Conference on Machine Learning (ICML)},
  year    = {2025},
  url     = {https://ibm.biz/ntl-main}
}
</pre>
        </div>

    </section>

    <!-- ▸▸▸ Footer ------------------------------------------------------ -->
    <footer class="py-4 text-center">
        <div class="container">
            <p>&copy; This webpage is inspired by <a href="https://dreamfusion3d.github.io/">DreamFusion</p>
        </div>
    </footer>

    <!-- ▸▸▸ Scripts ----------------------------------------------------- -->
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
</body>

</html>