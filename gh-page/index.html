<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Meta -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Regress, Don’t Guess – Number Token Loss</title>

    <!-- Fonts & CSS -->
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@300;400;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css">
    <link rel="stylesheet" href="assets/css/styles.css">

    <!-- Icons & Favicon -->
    <link rel="icon" href="assets/favicon.png" type="image/png">

    <!-- Social/SEO -->
    <meta property="og:site_name" content="Number Token Loss">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Regress, Don’t Guess – Number Token Loss">
    <meta property="og:description" content="A regression-like loss on number tokens for language models.">
    <meta property="og:url" content="https://tum-ai.github.io/number-token-loss">
    <meta property="og:image" content="assets/thumbnail.png">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Regress, Don’t Guess – Number Token Loss">
    <meta name="twitter:description" content="A regression-like loss on number tokens for language models.">
    <meta name="twitter:image" content="assets/thumbnail.png">

    <!-- Optional analytics -->
    <!--
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXX"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date()); gtag('config', 'G-XXXXXXX');
    </script>
    -->
    <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

<!-- ▸▸▸ Header ─────────────────────────────────────────────────────── -->
<header class="text-center py-4 bg-light">
    <div class="container">
        <h1 class="display-4 font-weight-bold">Regress, Don’t Guess<br><small class="h2">A Regression-like Loss on <span class="text-primary">N</span>umber <span class="text-primary">T</span>okens for <span class="text-primary">L</span>anguage Models</small></h1>
        <h3 class="mt-3">ICML 2025</h3>

        <!-- Author grid -->
        <div class="row justify-content-center mt-4">
            <!-- Repeat “col-md-2” blocks as needed; five per row works nicely -->
            <div class="col-10 col-sm-3 col-md-2">
                <h6><a href="https://scholar.google.com/citations?user=JPs9ufIAAAAJ&hl=en&oi=ao">Jonas Zausinger</a></h6>
                <p class="small">TUM.ai / TUM</p>
            </div>

            <div class="col-10 col-sm-3 col-md-2">
                <h6><a href="https://scholar.google.com/citations?user=CIjU6UgAAAAJ&hl=en&oi=ao">Lars Pennig</a></h6>
                <p class="small">TUM.ai / TUM / Helmholtz / MCML</p>
            </div>

            <div class="col-10 col-sm-3 col-md-2">
                <h6>Anamarija Kozina</h6>
                <p class="small">TUM.ai / TUM</p>
            </div>

            <div class="col-10 col-sm-3 col-md-2">
                <h6>Sean Sdahl</h6>
                <p class="small">TUM.ai / TUM</p>
            </div>

            <div class="col-10 col-sm-3 col-md-2">
                <h6>Julian Sikora</h6>
                <p class="small">TUM.ai / TUM</p>
            </div>



            <div class="col-10 col-sm-3 col-md-2">
                <h6><a href="https://scholar.google.com/citations?user=Flm5HnwAAAAJ&hl=en&oi=ao">Adrian Dendorfer</a></h6>
                <p class="small">TUM.ai / TUM</p>
            </div>
            <div class="col-10 col-sm-3 col-md-2">
                <h6>Timofey Kuznetsov</h6>
                <p class="small">TUM.ai / TUM</p>
            </div>
            <div class="col-10 col-sm-3 col-md-2">
                <h6>Mohamad Hagog</h6>
                <p class="small">TUM.ai / LMU</p>
            </div>
            <div class="col-10 col-sm-3 col-md-2">
                <h6><a href="https://scholar.google.com/citations?user=qC1JKzoAAAAJ&hl=en&oi=ao">Nina Wiedemann</a></h6>
                <p class="small">ETH Zurich</p>
            </div>
            <div class="col-10 col-sm-3 col-md-2">
                <h6>Kacper Chlodny</h6>
                <p class="small">TUM.ai / TUM</p>
            </div>
            <div class="col-10 col-sm-3 col-md-2">
                <h6>Vincent Limbach</h6>
                <p class="small">TUM.ai / TUM</p>
            </div>
            <div class="col-10 col-sm-3 col-md-2">
                <h6>Anna Ketteler</h6>
                <p class="small">TUM.ai / TUM</p>
            </div>
            <div class="col-10 col-sm-3 col-md-2">
                <h6><a href="https://scholar.google.com/citations?hl=en&user=qbX8sKwAAAAJ">Thorben Prein</a></h6>
                <p class="small">TUM.ai / TUM</p>
            </div>
            <div class="col-10 col-sm-3 col-md-2">
                <h6><a href="https://scholar.google.com/citations?hl=en&user=u5iBnt0AAAAJ">Vishwa Mohan Singh</a></h6>
                <p class="small">TUM.ai / LMU</p>
            </div>
            <div class="col-10 col-sm-3 col-md-2">
                <h6><a href="https://scholar.google.com/citations?hl=en&user=60LxTqgAAAAJ">Michael Danziger</a></h6>
                <p class="small">IBM Research Europe</p>
            </div>
            <div class="col-10 col-sm-3 col-md-2">
                <h6><a href="https://scholar.google.com/citations?hl=en&user=FHL-zfsAAAAJ">Jannis Born</a></h6>
                <p class="small">IBM Research Europe</p>
            </div>

            <!-- …continue for all co-authors, or cut after ~10 and add “et al.” -->
        </div>

        <!-- Action buttons -->
        <p class="lead mt-3">
            <a href="assets/paper.pdf" class="btn btn-outline-dark mx-1"><i class="fas fa-file-alt"></i> Paper</a>
            <a href="https://github.com/tum-ai/number-token-loss" class="btn btn-outline-dark mx-1"><i class="fab fa-github"></i> Code</a>
            <!-- <a href="#" class="btn btn-outline-dark mx-1"><i class="fas fa-video"></i> Video&nbsp;(coming soon)</a> -->
        </p>
    </div>
</header>

<section class="py-5" id="figure">
    <div class="container">
        <h5>The Number Token Loss (NTL) augments cross-entropy to encode numeric proximity at token level, improving language model performance in numerical tasks without harming general text capabilities or requiring additional compute.</h5>
    </div>
    <figure class="text-center my-4">
        <img src="assets/ntl_v4.svg" alt="Number Token Loss schematic" class="img-fluid" style="max-width:800px">
        <figcaption class="mt-2 small text-muted">The two flavors of the proposed Number Token Loss (NTL).</figcaption>
    </figure>
</section>

<!-- ▸▸▸ Abstract ────────────────────────────────────────────────────── -->
<section class="py-5 bg-light" id="abstract">
    <div class="container">
        <h2>Abstract</h2>
        <p class="text-justify">
            While language models have exceptional capabilities at text generation, they lack a natural inductive bias
            for emitting numbers and thus struggle in tasks involving quantitative reasoning, especially arithmetic.
            One fundamental limitation is the nature of the Cross Entropy loss, which assumes a nominal scale and thus
            cannot convey proximity between generated number tokens.
            In response, we here present a regression-like loss that operates purely on token level.
            Our proposed <em>Number Token Loss (NTL)</em> comes in two flavors and minimizes either the <em>L<sub>p</sub></em> norm or the Wasserstein distance between the
            <em>numerical values</em> of the real and predicted number tokens.
            NTL can easily be added to any language model and extend the Cross Entropy objective during training without runtime overhead.
            We evaluate the proposed scheme on various mathematical datasets and find that it consistently improves performance in math-related tasks.
            In a direct comparison on a regression task, we find that NTL can match the performance of a regression head, despite operating on token level.
            Finally, we scale NTL up to 3B parameter models and observe improved performance, demonstrating its potential for seamless integration into LLMs.
            We hope that this work can inspire LLM developers to improve their pretraining objectives.
        </p>
    </div>
</section>

<!-- ▸▸▸ Method / Figure 1 ------------------------------------------- -->
<section class="py-5 ">
    <div class="container">
        <h2>Method – Number Token Loss</h2>
        <p class="text-justify">
            The NTL allows computing a regression-like loss directly on a language modeling head.
            We propose two schemes:
            <br>
            <strong>NTL-MSE</strong> – dot-product expectation of numeric value with squared error, and<br>
            <strong>NTL-WAS</strong> – Wasserstein-1 distance between predicted and one-hot number distributions.
        </p>
        <!--
       <figure class="text-center my-4">
           <img src="assets/ntl_v4.svg" alt="Number Token Loss schematic" class="img-fluid" style="max-width:800px">
           <figcaption class="mt-2 small text-muted">The two flavors of the proposed Number Token Loss (NTL).</figcaption>
       </figure>
       -->
        CE is ordinal-scale and thus assigns equal loss to all incorrect predic-tions whereas NTL increases with distance from ground truth like
        a regression loss.
       <figure class="text-center my-4">
           <img src="assets/loss_comparison_v4.svg" alt="Number Token Loss VS. Cross Entropy" class="img-fluid" style="max-width:500px">
           <figcaption class="mt-2 small text-muted">Comparison of NTL to CE.</figcaption>
       </figure>
   </div>
</section>

<!-- ▸▸▸ Results summary (optional quick bullets) -------------------- -->
<section class="py-5 bg-light">
    <div class="container">
        <h2>Key Contributions & Results</h2>
        <ul>
            <li><strong>Model-agnostic:</strong> NTL can be applied to any language model (e.g., Transformer, Mamba) in any architecture (encoder-decoder, decoder-only).</li>
            <li><strong>Plug-and-play:</strong> NTL requires only a mapping from tokens to numeric values and works with digit-level and multi-digit tokenizations.</li>
            <li><strong>No computational overhead:</strong> The most advanced version (NTL-WAS) adds only ~1% compute to loss calculation, negligible over the full training step.</li>
            <li><strong>Consistently improves math performance:</strong> NTL outperforms CE across multiple architectures and mathematical benchmarks.</li>
            <li><strong>Matches regression-head performance:</strong> On regression tasks like rJokes, NTL matches dedicated regression heads despite operating on token level.</li>
            <li><strong>Does not harm textual understanding:</strong> On text-only tasks like MultiRC, NTL achieves equal performance to CE.</li>
            <li><strong>Scales to large models:</strong> T5-3B trained with NTL outperforms CE (GSM8k: 17.7% vs. 13.5%).</li>
        </ul>
    </div>
</section>

<!--
<section class="py-5">
    <div class="container">
        <h2>NTL at a Glance</h2>
        <p>
            The <strong>Number Token Loss (NTL)</strong> augments the cross-entropy loss to take the <em>numeric distance</em> between predicted and target number tokens into account.
            It is applied only to number tokens and is added to the usual token-level loss:
        </p>
        <p class="text-center mt-4 mb-4">
            \( \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{CE}} + \lambda \cdot \mathcal{L}_{\text{NTL}} \)
        </p>

        <p>We propose two variants of NTL:</p>

        <h5 class="mt-4"><strong>1. NTL-MSE:</strong> (Expectation-based squared error)</h5>
        <p>
            Interprets the number token distribution \( p \in \mathbb{R}^{|V_{\text{num}}|} \) as a soft prediction over values \( v \in \mathbb{R}^{|V_{\text{num}}|} \), and minimizes the squared error to the ground truth number \( y \in \mathbb{R} \):
        </p>
        <p class="text-center mt-3 mb-4">
            \( \mathcal{L}_{\text{NTL-MSE}} = \left( \sum_{i=1}^{|V_{\text{num}}|} p_i \cdot v_i - y \right)^2 \)
        </p>

        <h5 class="mt-4"><strong>2. NTL-WAS:</strong> (Wasserstein-1 distance)</h5>
        <p>
            Treats the target number token as a one-hot distribution and computes the discrete Wasserstein-1 (Earth Mover’s) distance between it and the predicted token distribution:
        </p>
        <p class="text-center mt-3 mb-4">
            \( \mathcal{L}_{\text{NTL-WAS}} = \sum_{i=1}^{|V_{\text{num}}|} \left| \text{CDF}_p(i) - \text{CDF}_q(i) \right| \)
        </p>
        <p class="text-muted small text-center">(where \( \text{CDF}_p \) and \( \text{CDF}_q \) are the cumulative distributions of the predicted and target distributions)</p>

        <p class="mt-4">
            Both losses can be implemented with negligible overhead and improve numeric reasoning in LMs. For full details, see Section 3 of the paper.
        </p>
    </div>
</section>
-->

<!-- ▸▸▸ Placeholder for extra figures or benchmarks ----------------- -->
<!--
<section class="py-5">
  <div class="container">
    <h2>Additional Figures</h2>
    <div class="row">
      <div class="col-md-6 text-center mb-4">
        <img src="assets/runtime_plot.svg" class="img-fluid" alt="Runtime">
        <p class="small mt-2">Figure 2: Marginal compute overhead of NTL.</p>
      </div>
      <div class="col-md-6 text-center mb-4">
        <img src="assets/maths_results.svg" class="img-fluid" alt="Math accuracy">
        <p class="small mt-2">Figure 3: Accuracy gains on maths tasks.</p>
      </div>
    </div>
  </div>
</section>
-->

<!-- ▸▸▸ Citation ----------------------------------------------------- -->

<section class="py-5 bg-light" id="citation">
    <div class="container">
        <h2>Citation</h2>
        <!--
        <pre class="bg-white p-3 rounded">
@inproceedings{zausinger2025regress,
  title   = {Regress, Don't Guess – A Regression-like Loss on Number Tokens for Language Models},
  author  = {Jonas Zausinger and Lars Pennig and Anamarija Kozina and Sean Sdahl
             and Julian Sikora and Adrian Dendorfer and Timofey Kuznetsov
             and Mohamad Hagog and Nina Wiedemann and Kacper Chlodny
             and Vincent Limbach and Anna Ketteler and Thorben Prein
             and Vishwa Mohan Singh and Michael Danziger and Jannis Born},
  booktitle = {Proc. of the 41st International Conference on Machine Learning (ICML)},
  year    = {2025},
  url     = {https://github.com/tum-ai/number-token-loss}
}
</pre>-->
    </div>

</section>

<!-- ▸▸▸ Footer ------------------------------------------------------ -->
<footer class="py-4 text-center">
    <div class="container">
        <p>&copy; This webpage is inspired by <a href="https://dreamfusion3d.github.io/">DreamFusion</p>
    </div>
</footer>

<!-- ▸▸▸ Scripts ----------------------------------------------------- -->
<script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
</body>
</html>
